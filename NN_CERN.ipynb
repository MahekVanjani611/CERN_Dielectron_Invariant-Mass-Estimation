{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anMiFeupFFnc",
        "outputId": "ab5b3889-037c-4f70-82da-5b1f628ff704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run      0\n",
            "Event    0\n",
            "E1       0\n",
            "px1      0\n",
            "py1      0\n",
            "pz1      0\n",
            "pt1      0\n",
            "eta1     0\n",
            "phi1     0\n",
            "Q1       0\n",
            "E2       0\n",
            "px2      0\n",
            "py2      0\n",
            "pz2      0\n",
            "pt2      0\n",
            "eta2     0\n",
            "phi2     0\n",
            "Q2       0\n",
            "M        0\n",
            "dtype: int64\n",
            "Epoch [1/30], Learning Rate: 0.010000, Training Loss: 57.5316, Validation Loss: 22.3995\n",
            "Epoch [2/30], Learning Rate: 0.010000, Training Loss: 21.2112, Validation Loss: 19.3913\n",
            "Epoch [3/30], Learning Rate: 0.010000, Training Loss: 31.4446, Validation Loss: 10.9130\n",
            "Epoch [4/30], Learning Rate: 0.010000, Training Loss: 51.7479, Validation Loss: 10.4113\n",
            "Epoch [5/30], Learning Rate: 0.010000, Training Loss: 150.5732, Validation Loss: 10.3988\n",
            "Epoch [6/30], Learning Rate: 0.010000, Training Loss: 27.2299, Validation Loss: 7.4708\n",
            "Epoch [7/30], Learning Rate: 0.010000, Training Loss: 75.2007, Validation Loss: 11.8513\n",
            "Epoch [8/30], Learning Rate: 0.010000, Training Loss: 8.0120, Validation Loss: 16.1064\n",
            "Epoch [9/30], Learning Rate: 0.010000, Training Loss: 21.7725, Validation Loss: 10.8149\n",
            "Epoch [10/30], Learning Rate: 0.010000, Training Loss: 29.2985, Validation Loss: 13.6734\n",
            "Epoch [11/30], Learning Rate: 0.010000, Training Loss: 15.6266, Validation Loss: 5.4362\n",
            "Epoch [12/30], Learning Rate: 0.010000, Training Loss: 8.9536, Validation Loss: 6.1232\n",
            "Epoch [13/30], Learning Rate: 0.010000, Training Loss: 19.8822, Validation Loss: 4.6242\n",
            "Epoch [14/30], Learning Rate: 0.010000, Training Loss: 67.3662, Validation Loss: 9.1918\n",
            "Epoch [15/30], Learning Rate: 0.010000, Training Loss: 98.1130, Validation Loss: 6.1942\n",
            "Epoch [16/30], Learning Rate: 0.010000, Training Loss: 83.6215, Validation Loss: 8.5434\n",
            "Epoch [17/30], Learning Rate: 0.010000, Training Loss: 64.3976, Validation Loss: 6.5160\n",
            "Epoch [18/30], Learning Rate: 0.010000, Training Loss: 27.2616, Validation Loss: 5.0550\n",
            "Early stopping! No improvement in validation loss for 5 epochs.\n",
            "Test Loss: 5.1154\n",
            "XGBoost Test MSE: 13.3757\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Assuming all imports are done correctly, let's proceed with the modifications and debugging.\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/dielectron.csv')\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "print(data.isna().sum())\n",
        "\n",
        "# Dropping 'Run' and 'Event' columns\n",
        "data.drop(columns=['Run', 'Event'], inplace=True)\n",
        "\n",
        "# Separating features and target variable\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Defining Neural Network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Training Neural Network\n",
        "input_size = X_train.shape[1]\n",
        "model = NeuralNet(input_size)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
        "\n",
        "num_epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "# Splitting the training data further into train and validation sets\n",
        "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 5  # Define early stopping patience\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader:\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_targets.unsqueeze(1)).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler.step(val_loss)  # Move this line inside the training loop\n",
        "\n",
        "    # Print training and validation information\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}, '\n",
        "          f'Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(f'Early stopping! No improvement in validation loss for {early_stopping_patience} epochs.')\n",
        "        break\n",
        "\n",
        "# Testing\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for test_inputs, test_targets in test_loader:\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss += criterion(test_outputs, test_targets.unsqueeze(1)).item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "# Training XGBoost model\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions using XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculating mean squared error for XGBoost model\n",
        "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
        "print(f'XGBoost Test MSE: {xgb_mse:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5hLWbuNFSjf",
        "outputId": "5746208e-10e7-497d-adf8-d374ea6a4772"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S11-og3wFY_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}